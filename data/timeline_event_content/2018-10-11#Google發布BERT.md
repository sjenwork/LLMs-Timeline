**Google發布BERT**
時間：2018-10-11
Google發布BERT (Bidirectional Encoder Representations from Transformers)，改進了雙向編碼表示，成為自然語言處理的重要里程碑。

**核心創新：**

* 真正的雙向性，同時考慮詞語左右兩側的上下文
* 遮罩語言模型(MLM)預訓練任務
* 下一句預測(NSP)任務
* 基於Transformer的Encoder架構

**模型規格：**

* BERT-Base: 12層Transformer編碼器，參數量約1.1億
* BERT-Large: 24層Transformer編碼器，參數量約3.4億

**參考資料：**

* [研究論文](https://arxiv.org/abs/1810.04805)
* [Google AI Blog介紹](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)