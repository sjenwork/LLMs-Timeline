{
    "title": {
        "media": {
            "url": "https://images.unsplash.com/photo-1620712943543-bcc4688e7485?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1740&q=80",
            "caption": "大型語言模型發展時間線",
            "credit": "AI Technology Evolution"
        },
        "text": {
            "headline": "大型語言模型發展時間線",
            "text": "<p>本時間線記錄了從2017年Transformer架構提出到2025年AI Agent技術崛起的重要里程碑。</p>"
        }
    },
    "events": [
        {
            "start_date": {
                "year": "2017",
                "month": "6",
                "day": "12"
            },
            "media": {
                "url": "https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png",
                "caption": "Transformer架構示意圖",
                "credit": "Google"
            },
            "text": {
                "headline": "Google發布Transformer架構",
                "text": "Google在*arXiv*上發表論文《Attention Is All You Need》，提出了革命性的Transformer架構。\n\n**核心創新：**\n\n* 完全基於注意力機制(Attention Mechanism)\n* 摒棄了傳統循環神經網路(RNN)和卷積神經網路(CNN)的遞迴結構\n* 實現並行計算，大幅提高訓練效率和模型性能\n\n**參考資料：**\n\n* [原始論文](https://arxiv.org/abs/1706.03762)\n* [Google AI Blog介紹](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)"
            }
        },
        {
            "start_date": {
                "year": "2018",
                "month": "6",
                "day": "11"
            },
            "media": {
                "url": "https://pic.616pic.com/ys_img/00/58/25/Tq57QjfSmP.jpg",
                "caption": "GPT-1架構",
                "credit": "OpenAI"
            },
            "text": {
                "headline": "OpenAI發布GPT-1",
                "text": "OpenAI發布首個GPT模型，參數量約1.17億，是首個將Transformer與無監督預訓練結合的大型語言模型。\n\n**核心特點：**\n\n* 基於Transformer的12層Decoder-only架構\n* 在BooksCorpus上進行無監督預訓練\n* 通過預測下一個詞來學習語言模式\n* 確立了「大規模無監督預訓練 + 特定任務微調」的技術路線\n\n**參考資料：**\n\n* [研究論文](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n* [OpenAI官方介紹](https://openai.com/index/language-unsupervised/)"
            }
        },
        {
            "start_date": {
                "year": "2018",
                "month": "10",
                "day": "11"
            },
            "media": {
                "url": "https://pic.616pic.com/ys_img/00/58/25/Tq57QjfSmP.jpg",
                "caption": "BERT預訓練與微調過程示意圖",
                "credit": "Google"
            },
            "text": {
                "headline": "Google發布BERT",
                "text": "Google發布BERT (Bidirectional Encoder Representations from Transformers)，改進了雙向編碼表示，成為自然語言處理的重要里程碑。\n\n**核心創新：**\n\n* 真正的雙向性，同時考慮詞語左右兩側的上下文\n* 遮罩語言模型(MLM)預訓練任務\n* 下一句預測(NSP)任務\n* 基於Transformer的Encoder架構\n\n**模型規格：**\n\n* BERT-Base: 12層Transformer編碼器，參數量約1.1億\n* BERT-Large: 24層Transformer編碼器，參數量約3.4億\n\n**參考資料：**\n\n* [研究論文](https://arxiv.org/abs/1810.04805)\n* [Google AI Blog介紹](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)"
            }
        },
        {
            "start_date": {
                "year": "2019",
                "month": "2",
                "day": "14"
            },
            "media": {
                "url": "https://cdn.openai.com/research-covers/language-unsupervised/2x-no-mark.jpg",
                "caption": "GPT-2生成的文本示例",
                "credit": "OpenAI"
            },
            "text": {
                "headline": "OpenAI發布GPT-2",
                "text": "OpenAI發布GPT-2，參數量達15億，質量明顯提升，但因安全考慮初期僅有限發布。\n\n**核心進展：**\n\n* 參數規模擴大至15億\n* 訓練數據更多樣化(WebText)\n* 顯著提升文本生成質量和連貫性\n* 展現出零樣本學習能力\n\n**參考資料：**\n\n* [OpenAI研究介紹](https://openai.com/research/better-language-models)"
            }
        },
        {
            "start_date": {
                "year": "2020",
                "month": "5",
                "day": "28"
            },
            "media": {
                "url": "https://pic.616pic.com/ys_img/00/58/25/Tq57QjfSmP.jpg",
                "caption": "GPT-3架構示意圖",
                "credit": "OpenAI"
            },
            "text": {
                "headline": "OpenAI發布GPT-3",
                "text": "OpenAI發布GPT-3，參數量達1750億，標誌著大型語言模型規模的重大飛躍，展現出上下文學習能力。\n\n**核心突破：**\n\n* 參數規模達1750億，是當時最大的語言模型\n* 強大的少樣本學習能力(Few-shot Learning)\n* 上下文學習能力顯著提升\n* 通過API商業化，開啟新的AI即服務模式\n\n**技術細節：**\n\n* 96層Transformer Decoder架構\n* 訓練數據包括Common Crawl、WebText2、Books1、Books2和Wikipedia\n\n**參考資料：**\n\n* [研究論文](https://arxiv.org/abs/2005.14165)\n* [OpenAI介紹](https://openai.com/research/gpt-3-apps)"
            }
        },
        {
            "start_date": {
                "year": "2020",
                "month": "10",
                "day": "20"
            },
            "media": {
                "url": "img/T5.jpg",
                "caption": "T5統一的文本到文本框架",
                "credit": "Google"
            },
            "text": {
                "headline": "Google發布T5模型",
                "text": "Google發布T5 (Text-to-Text Transfer Transformer) 模型，統一了多任務訓練框架。\n\n**核心創新：**\n\n* 統一的「文本到文本」框架將所有NLP任務轉換為相同格式\n* 基於標準的Encoder-Decoder Transformer架構\n* 新的預訓練目標：跨度破壞(Span Corruption)\n* 大規模、乾淨的預訓練數據集(C4)\n\n**模型規格：**\n\n* 多種參數規模，從Small到11B(110億)參數\n\n**參考資料：**\n\n* [研究論文](https://arxiv.org/abs/1910.10683)\n* [Google AI Blog介紹](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)"
            }
        },
        {
            "start_date": {
                "year": "2021",
                "month": "1",
                "day": "5"
            },
            "media": {
                "url": "img/Dall-E.webp",
                "caption": "DALL·E生成的圖像示例",
                "credit": "OpenAI"
            },
            "text": {
                "headline": "OpenAI發布DALL·E和CLIP",
                "text": "OpenAI發布DALL·E和CLIP，分別用於圖像生成和圖文理解，標誌著多模態AI技術的重要進展。\n\n**DALL·E特點：**\n\n* 基於GPT-3架構的變體\n* 能夠從文本描述生成創意圖像\n* 理解複雜的視覺概念和組合\n\n**CLIP特點：**\n\n* Contrastive Language-Image Pre-training\n* 學習圖像和文本之間的關係\n* 零樣本遷移至各種視覺任務\n\n**參考資料：**\n\n* [DALL·E介紹](https://openai.com/research/dall-e)\n* [CLIP介紹](https://openai.com/research/clip)"
            }
        },
        {
            "start_date": {
                "year": "2022",
                "month": "11",
                "day": "30"
            },
            "media": {
                "url": "img/ChatGPT.png",
                "caption": "ChatGPT界面示例",
                "credit": "OpenAI"
            },
            "text": {
                "headline": "OpenAI發布ChatGPT",
                "text": "OpenAI發布ChatGPT，基於GPT-3.5系列，通過RLHF優化，提供更自然的對話體驗，標誌著生成式AI進入大眾視野。\n\n**核心特點：**\n\n* 基於GPT-3.5系列模型\n* 使用人類反饋強化學習(RLHF)進行優化\n* 更好的指令遵循能力\n* 有害內容生成的降低\n* 對話能力的大幅提升\n\n**社會影響：**\n\n* 5天內用戶量突破百萬\n* 成為歷史上增長最快的消費應用之一\n* 推動了整個生成式AI行業的爆發\n\n**參考資料：**\n\n* [OpenAI官方介紹](https://openai.com/blog/chatgpt)"
            }
        },
        {
            "start_date": {
                "year": "2023",
                "month": "3",
                "day": "14"
            },
            "media": {
                "url": "img/GPT4.jpg",
                "caption": "GPT-4性能對比圖",
                "credit": "OpenAI"
            },
            "text": {
                "headline": "OpenAI發布GPT-4",
                "text": "OpenAI發布GPT-4，支持多模態輸入，能夠分析文本和圖像，推理能力和安全性顯著提升。\n\n**核心突破：**\n\n* 支持多模態輸入，能夠理解和分析圖像\n* 複雜推理能力大幅提升\n* 上下文窗口擴展(初始為8K，後擴展至32K)\n* 更強的指令遵循能力\n* 在專業考試和基準測試上表現優異\n\n**參考資料：**\n\n* [OpenAI研究介紹](https://openai.com/research/gpt-4)"
            }
        },
        {
            "start_date": {
                "year": "2023",
                "month": "12",
                "day": "6"
            },
            "media": {
                "url": "img/Gemini 1.0.jpg",
                "caption": "Gemini模型示意圖",
                "credit": "Google"
            },
            "text": {
                "headline": "Google發布Gemini 1.0系列",
                "text": "Google發布Gemini 1.0系列，包括Ultra、Pro和Nano三個版本，是Google首個原生多模態大型語言模型。\n\n**核心特點：**\n\n* 原生多模態設計(非模型拼接)\n* 優化的Transformer解碼器架構\n* 同時處理文本、圖像、音頻、視頻等多模態數據\n* 大規模多任務訓練\n\n**版本規格：**\n\n* Ultra: 最強大的版本，在多項基準測試上超越GPT-4\n* Pro: D平衡能力與效率的版本，用於Google Bard\n* Nano: 輕量級版本，可在移動設備上運行\n\n**參考資料：**\n\n* [Google官方介紹](https://blog.google/technology/ai/google-gemini-ai/)"
            }
        },
        {
            "start_date": {
                "year": "2024",
                "month": "1",
                "day": "29"
            },
            "media": {
                "url": "img/Claude 3.jpg",
                "caption": "Claude 3系列模型示意圖",
                "credit": "Anthropic"
            },
            "text": {
                "headline": "Anthropic發布Claude 3系列",
                "text": "Anthropic發布Claude 3系列，包括Haiku、Sonnet和Opus三個版本，在多個基準測試中表現卓越。\n\n**核心特點：**\n\n* 首次支持圖像輸入的多模態能力\n* 視覺推理顯著增強\n* 更低的幻覺率\n* 更強的問題解決能力\n\n**版本規格：**\n\n* Opus: 最強大版本，性能超越大多數競爭模型\n* Sonnet: 平衡性能與速度的中等版本\n* Haiku: 快速輕量版本，適合高性能需求場景\n\n**參考資料：**\n\n* [Anthropic官方介紹](https://www.anthropic.com/news/claude-3-family)"
            }
        },
        {
            "start_date": {
                "year": "2024",
                "month": "2",
                "day": "15"
            },
            "media": {
                "url": "img/Gemini 1.5 Pro.png",
                "caption": "Gemini 1.5 Pro超長上下文示意圖",
                "credit": "Google"
            },
            "text": {
                "headline": "Google發布Gemini 1.5 Pro",
                "text": "Google發布Gemini 1.5 Pro，提供100萬令牌的上下文窗口，建立了長上下文處理的新標準。\n\n**核心突破：**\n\n* 超長上下文窗口：支持100萬令牌的上下文處理\n* 視頻理解增強：能夠處理和理解長視頻內容\n* 代碼處理能力提升：可以分析和理解大型代碼庫\n\n**架構改進：**\n\n* 混合注意力機制(可能採用稀疏注意力技術)\n* 更高效的上下文壓縮方法\n\n**參考資料：**\n\n* [Google官方介紹](https://blog.google/technology/ai/google-gemini-1-5-pro-availability/)"
            }
        },
        {
            "start_date": {
                "year": "2024",
                "month": "5",
                "day": "14"
            },
            "media": {
                "url": "img/gpt-4o.png",
                "caption": "GPT-4o多模態能力示例",
                "credit": "OpenAI"
            },
            "text": {
"headline": "OpenAI發布GPT-4o",
                "text": "OpenAI發布GPT-4o ('o' for 'omni')，進一步強化多模態處理能力，響應速度更快，用戶體驗更自然。\n\n**核心特性：**\n\n* 強化的多模態處理能力(文本、圖像、音頻)\n* 大幅提升的響應速度\n* 更自然的對話體驗\n* 實時語音交互和視覺理解能力\n\n**參考資料：**\n\n* [OpenAI官方介紹](https://openai.com/blog/gpt-4o)"
            }
        },
        {
            "start_date": {
                "year": "2024",
                "month": "11",
                "day": "15"
            },
            "media": {
                "url": "img/MCP.png",
                "caption": "MCP架構示意圖",
                "credit": "Anthropic"
            },
            "text": {
                "headline": "Anthropic發布Model Context Protocol (MCP)",
                "text": "Anthropic發布Model Context Protocol (MCP)，為AI系統提供統一標準，使模型能夠安全訪問工具和外部數據。\n\n**MCP核心定義：**\n\n* 一個開放標準，定義應用程序如何向大型語言模型提供結構化上下文\n* 創建模型與外部工具、API和數據源之間的安全雙向連接\n\n**MCP工作原理：**\n\n* MCP主機：作為中央樞紐的應用程序\n* MCP服務器：暴露特定功能的服務\n* MCP客戶端：通常是由語言模型驅動的應用程序\n* 數據源連接：本地和遠程數據源的標準訪問方式\n\n**行業意義：**\n\n* 開創了Agent發展的新階段\n* 建立AI工具使用的共同標準"
            }
        },
        {
            "start_date": {
                "year": "2025",
                "month": "1",
                "day": "23"
            },
            "media": {
                "url": "img/openAI Operator.webp",
                "caption": "OpenAI Operator示意圖",
                "credit": "OpenAI"
            },
            "text": {
                "headline": "OpenAI發布Operator",
                "text": "OpenAI發布Operator，由Computer-Using Agent (CUA)驅動，成為OpenAI發布的第一個AI Agent。\n\n**核心技術：**\n\n* Computer-Using Agent (CUA)\n\n**關鍵能力：**\n\n* 能夠自主使用計算機執行任務\n* 網頁瀏覽和交互\n* 文件操作和數據處理\n* 完成複雜的多步驟任務"
            }
        },
        {
            "start_date": {
                "year": "2025",
                "month": "2",
                "day": "14"
            },
            "media": {
                "url": "img/Claude 3.7 Sonnet.webp",
                "caption": "Claude 3.7 Sonnet思考模式示意圖",
                "credit": "Anthropic"
            },
            "text": {
                "headline": "Anthropic發布Claude 3.7 Sonnet",
                "text": "Anthropic發布Claude 3.7 Sonnet，引入混合推理方法，在單一系統中結合快速響應和更深入的逐步思考。\n\n**關鍵創新：**\n\n* 混合推理(Hybrid Reasoning)\n* 結合快速、直覺式反應(System 1)與深度、逐步思考(System 2)\n* 在處理複雜問題時能夠切換到更深入的推理模式\n* 被用戶稱為思考模式(Thinking Mode)\n\n**能力提升：**\n\n* 程式碼理解和生成能力顯著增強\n* 複雜問題解決能力提升\n* 更透明的推理過程展示"
            }
        },
        {
            "start_date": {
                "year": "2025",
                "month": "4",
                "day": "9"
            },
            "media": {
                "url": "img/A2A.png",
                "caption": "A2A協議架構示意圖",
                "credit": "Google"
            },
            "text": {
                "headline": "Google發布Agent-to-Agent (A2A)協議",
                "text": "Google發布Agent-to-Agent (A2A)協議，用於AI代理之間的直接通信，與MCP形成互補關係。\n\n**A2A關鍵特性：**\n\n* 代理發現：通過AgentCard機制讓代理互相發現\n* 任務協調：支持從快速操作到長時間運行的流程\n* 安全協作：具有企業級身份驗證\n* 基於任務的狀態管理\n\n**與MCP的關係：**\n\n* A2A專注於代理之間的通信\n* MCP專注於代理與工具的連接\n* 兩者形成互補關係\n\n**行業意義：**\n\n* Google同時宣布支持Anthropic的MCP協議\n* 推動行業標準形成\n* 展示大型AI公司在標準方面的合作"
            }
        }
    ]
}                